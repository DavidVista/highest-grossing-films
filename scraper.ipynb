{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EFqPPMBkHBh"
      },
      "source": [
        "## Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwPR1zZkHBi"
      },
      "source": [
        "## Install necessary dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzb-2PQukHBj",
        "outputId": "8e39a1db-85a8-446c-b739-4a6d746f6c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.12.2)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Downloading pymongo-4.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymongo\n",
            "Successfully installed pymongo-4.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install bs4\n",
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbt1kVrMkHBj"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "QH0mM7kVkHBk"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen, urlparse\n",
        "from urllib.error import HTTPError, URLError\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Tuple, List, TypedDict, Optional, Any, NamedTuple\n",
        "import re\n",
        "from pymongo import MongoClient\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "fJ_YUnX6kHBk"
      },
      "outputs": [],
      "source": [
        "TARGET_URL = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n",
        "BASE_URL = '://'.join(urlparse(TARGET_URL)[:2])\n",
        "\n",
        "def extract_html(url: str) -> Any:\n",
        "    try:\n",
        "        return urlopen(url)\n",
        "    except HTTPError as e:\n",
        "        print(e.__str__())\n",
        "    except URLError:\n",
        "        print(\"The server could not be found\")\n",
        "    return None\n",
        "\n",
        "def exclude_refs(name: str) -> str:\n",
        "    pos = name.find('[')\n",
        "    return name[:pos] if pos > 0 else name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PM5PWkdwkHBl"
      },
      "outputs": [],
      "source": [
        "if (html := extract_html(TARGET_URL)) is not None:\n",
        "    main_page = BeautifulSoup(html, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z-AD1d0-kHBl"
      },
      "outputs": [],
      "source": [
        "highest_grossing_films = main_page.find('table', {'class': 'wikitable plainrowheaders sticky-header col4right col5center col6center'})\n",
        "assert highest_grossing_films is not None\n",
        "assert len(highest_grossing_films.find_all('tr')) != 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "WIHUv457kHBl"
      },
      "outputs": [],
      "source": [
        "class FilmRecord(TypedDict):\n",
        "    title: str\n",
        "    release_year: Optional[int]\n",
        "    director: Optional[str]\n",
        "    box_office: Optional[float]\n",
        "    country: Optional[str]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(extract_html(\"https://en.wikipedia.org/wiki/Captain_Marvel_(film)\"), 'html.parser')\n",
        "table = soup.find('table', {'class': 'infobox vevent'})\n",
        "directors = table.find(lambda tag: re.compile('^\\s*Directed by').match(tag.text))\n",
        "# directors_list_element = directors.find_all('div', {'class': 'plainlist'})[-1]\n",
        "# directors = ';'.join(director_element.text for director_element in directors_list_element.find_all('li'))\n",
        "# print(directors)\n",
        "annotated_text = directors.find_all()[1].get_text(strip=True, separator='\\n')\n",
        "directors_list = re.split(\"\\[[^\\]]*\\]|\\n\", annotated_text)\n",
        "directors = ';'.join(directors_list)\n",
        "print(directors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiB8w76gb7T0",
        "outputId": "10d4bb5c-ba04-447a-a59f-26870ac75a1f"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anna Boden;Ryan Fleck\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "gpXQ0YELkHBl"
      },
      "outputs": [],
      "source": [
        "def parse_revenue(revenue: str) -> Optional[float]:\n",
        "    revenue = exclude_refs(revenue)\n",
        "    quantity, order = revenue.split()\n",
        "    value = quantity[re.search(\"[\\d\\.]+\", quantity).start():]\n",
        "    if (order == 'million'):\n",
        "        major = ''.join(value.split('.'))\n",
        "        digits_after_decimal = len(value.split('.')[1])\n",
        "        return float(major + '0' * (6-digits_after_decimal))\n",
        "    elif (order == 'billion'):\n",
        "        major = ''.join(value.split('.'))\n",
        "        digits_after_decimal = len(value.split('.')[1])\n",
        "        return float(major + '0' * (9-digits_after_decimal))\n",
        "    else:\n",
        "        print(f\"Unresolved order: {order}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_film_page(film_url: str) -> Optional[Tuple[str, float, str]]:\n",
        "    if (html := extract_html(film_url)) is None:\n",
        "      return None\n",
        "    film_information = BeautifulSoup(html, 'html.parser').find('table', {'class': 'infobox vevent'})\n",
        "\n",
        "    # Directors\n",
        "    directors_row_element = film_information.find(lambda tag: re.compile('^\\s*[Dd]irected\\s+by').match(tag.text) is not None)\n",
        "    if directors_row_element is not None:\n",
        "        if directors_row_element.find('div', {'class': 'plainlist'}) is not None:\n",
        "            directors_list_element = directors_row_element.find_all('div', {'class': 'plainlist'})[-1]\n",
        "            directors = ';'.join(exclude_refs(director_element.text) for director_element in directors_list_element.find_all('li'))\n",
        "        else:\n",
        "            annotated_text = directors_row_element.find_all()[1].get_text(strip=True, separator='\\n')\n",
        "            directors_list = re.split(\"\\[[^\\]]*\\]|\\n\", annotated_text)\n",
        "            directors = ';'.join(directors_list)\n",
        "    else:\n",
        "        print(f'Film (url={film_url}): directors list is not found')\n",
        "        directors = None\n",
        "\n",
        "    # Box office revenue\n",
        "    box_office_revenue_row_element = film_information.find(lambda tag: re.compile('^\\s*[Bb]ox\\s+office').match(tag.text) is not None)\n",
        "    if box_office_revenue_row_element is not None:\n",
        "        if (box_office_revenue_element:=box_office_revenue_row_element.find('td', {'class': 'infobox-data'})) is not None:\n",
        "            box_office_revenue = parse_revenue(box_office_revenue_element.text.strip())\n",
        "        else:\n",
        "            print(f'Film (url={film_url}): box revenue is not found')\n",
        "            box_office_revenue = None\n",
        "    else:\n",
        "        print(f'Film (url={film_url}): box revenue is not found')\n",
        "        box_office_revenue = None\n",
        "\n",
        "    # Countries\n",
        "    countries_row_element = film_information.find(lambda tag: re.compile('^\\s*[Cc]ountry|[Cc]ountries').match(tag.text) is not None)\n",
        "    if countries_row_element is not None:\n",
        "        if countries_row_element.find('div', {'class': 'plainlist'}) is not None:\n",
        "            country_list_element = countries_row_element.find_all('div', {'class': 'plainlist'})[-1]\n",
        "            countries = ';'.join(exclude_refs(country_element.text) for country_element in country_list_element.find_all('li'))\n",
        "        else:\n",
        "            annotated_text = countries_row_element.find_all()[1].get_text(strip=True, separator='\\n')\n",
        "            countries_list = re.split(\"\\[[^\\]]*\\]|\\n\", annotated_text)\n",
        "            countries = ';'.join(countries_list)\n",
        "    else:\n",
        "        print(f'Film (url={film_url}): countries list is not found')\n",
        "        countries = None\n",
        "\n",
        "    return (directors, box_office_revenue, countries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "jrUdq27-kHBm"
      },
      "outputs": [],
      "source": [
        "films: List[FilmRecord] = []\n",
        "film_rows = highest_grossing_films.find_all('tr')[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4yz0j8skHBm",
        "outputId": "5c286d1c-05ba-4223-a91a-e453b724192c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 0: Started processing\n",
            "Row 0: parsed a film\n",
            "Row 1: Started processing\n",
            "Row 1: parsed a film\n",
            "Row 2: Started processing\n",
            "Row 2: parsed a film\n",
            "Row 3: Started processing\n",
            "Row 3: parsed a film\n",
            "Row 4: Started processing\n",
            "Row 4: parsed a film\n",
            "Row 5: Started processing\n",
            "Row 5: parsed a film\n",
            "Row 6: Started processing\n",
            "Row 6: parsed a film\n",
            "Row 7: Started processing\n",
            "Row 7: parsed a film\n",
            "Row 8: Started processing\n",
            "Row 8: parsed a film\n",
            "Row 9: Started processing\n",
            "Row 9: parsed a film\n",
            "Row 10: Started processing\n",
            "Row 10: parsed a film\n",
            "Row 11: Started processing\n",
            "Row 11: parsed a film\n",
            "Row 12: Started processing\n",
            "Row 12: parsed a film\n",
            "Row 13: Started processing\n",
            "Row 13: parsed a film\n",
            "Row 14: Started processing\n",
            "Row 14: parsed a film\n",
            "Row 15: Started processing\n",
            "Row 15: parsed a film\n",
            "Row 16: Started processing\n",
            "Row 16: parsed a film\n",
            "Row 17: Started processing\n",
            "Row 17: parsed a film\n",
            "Row 18: Started processing\n",
            "Row 18: parsed a film\n",
            "Row 19: Started processing\n",
            "Row 19: parsed a film\n",
            "Row 20: Started processing\n",
            "Row 20: parsed a film\n",
            "Row 21: Started processing\n",
            "Row 21: parsed a film\n",
            "Row 22: Started processing\n",
            "Row 22: parsed a film\n",
            "Row 23: Started processing\n",
            "Row 23: parsed a film\n",
            "Row 24: Started processing\n",
            "Row 24: parsed a film\n",
            "Row 25: Started processing\n",
            "Row 25: parsed a film\n",
            "Row 26: Started processing\n",
            "Row 26: parsed a film\n",
            "Row 27: Started processing\n",
            "Row 27: parsed a film\n",
            "Row 28: Started processing\n",
            "Row 28: parsed a film\n",
            "Row 29: Started processing\n",
            "Row 29: parsed a film\n",
            "Row 30: Started processing\n",
            "Row 30: parsed a film\n",
            "Row 31: Started processing\n",
            "Row 31: parsed a film\n",
            "Row 32: Started processing\n",
            "Row 32: parsed a film\n",
            "Row 33: Started processing\n",
            "Row 33: parsed a film\n",
            "Row 34: Started processing\n",
            "Row 34: parsed a film\n",
            "Row 35: Started processing\n",
            "Row 35: parsed a film\n",
            "Row 36: Started processing\n",
            "Row 36: parsed a film\n",
            "Row 37: Started processing\n",
            "Row 37: parsed a film\n",
            "Row 38: Started processing\n",
            "Row 38: parsed a film\n",
            "Row 39: Started processing\n",
            "Row 39: parsed a film\n",
            "Row 40: Started processing\n",
            "Row 40: parsed a film\n",
            "Row 41: Started processing\n",
            "Row 41: parsed a film\n",
            "Row 42: Started processing\n",
            "Row 42: parsed a film\n",
            "Row 43: Started processing\n",
            "Row 43: parsed a film\n",
            "Row 44: Started processing\n",
            "Row 44: parsed a film\n",
            "Row 45: Started processing\n",
            "Row 45: parsed a film\n",
            "Row 46: Started processing\n",
            "Row 46: parsed a film\n",
            "Row 47: Started processing\n",
            "Row 47: parsed a film\n",
            "Row 48: Started processing\n",
            "Row 48: parsed a film\n",
            "Row 49: Started processing\n",
            "Row 49: parsed a film\n"
          ]
        }
      ],
      "source": [
        "for i, row in enumerate(film_rows):\n",
        "    print(f\"Row {i}: Started processing\")\n",
        "    elements = row.find_all(recursive=False)\n",
        "    assert len(elements) == 6\n",
        "\n",
        "    # Title collection\n",
        "    title_element = elements[2].find('a')\n",
        "    if title_element is None:\n",
        "        print(f\"Row {i}: title element was not found, the row is excluded\")\n",
        "        continue\n",
        "    title_link = title_element.attrs['href']\n",
        "    title = title_element.text.strip()\n",
        "\n",
        "    # Year collection\n",
        "    year_element = elements[4]\n",
        "    if len(year_element.text.strip()) == 0:\n",
        "        print(f\"Row {i}: release year is missing\")\n",
        "    try:\n",
        "        release_year = int(year_element.text.strip())\n",
        "    except ValueError:\n",
        "        release_year = None\n",
        "        print(f\"Row {i}: invalid year format: {year_element.text.strip()}\")\n",
        "\n",
        "    # Moving to film page\n",
        "    film_url = BASE_URL + title_link\n",
        "    director, box_office, country = parse_film_page(film_url)\n",
        "    print(f\"Row {i}: parsed a film\")\n",
        "    films.append(FilmRecord(title=title, release_year=release_year, director=director, box_office=box_office, country=country))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Database creation"
      ],
      "metadata": {
        "id": "xGjIRzqL3LVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the placeholders with your actual MongoDB Atlas credentials\n",
        "username = \"pyclient\"\n",
        "password = \"admin\"\n",
        "URL = \"@mycluster.hszuy.mongodb.net/?retryWrites=true&w=majority&appName=MyCluster\"\n",
        "\n",
        "# Construct the MongoDB URI with authentication details\n",
        "mongo_uri = f\"mongodb+srv://{username}:{password}{URL}\"\n",
        "\n",
        "# Create a MongoClient object with the URI\n",
        "client = MongoClient(mongo_uri)"
      ],
      "metadata": {
        "id": "r2Ud2_h2q861"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = client[\"wikipedia\"]\n",
        "if \"highest_grossing\" not in db.list_collection_names():\n",
        "  collection = db[\"highest_grossing\"]\n",
        "  for film in films:\n",
        "      collection.insert_one(film)\n",
        "else:\n",
        "  collection = db[\"highest_grossing\"]\n",
        "  # collection.drop()"
      ],
      "metadata": {
        "id": "X1OKMoFsq-Ux"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting to JSON"
      ],
      "metadata": {
        "id": "ZrDqiZNB8woL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cursor = collection.find()\n",
        "\n",
        "# Converting cursor to the list of dictionaries\n",
        "list_cur = list(cursor)\n",
        "for film in list_cur:\n",
        "  film.pop('_id')\n",
        "json_data = json.dumps(list_cur, indent=4, ensure_ascii=False)\n",
        "\n",
        "with open('data.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json_data)"
      ],
      "metadata": {
        "id": "Gc0h9Jcl6ADJ"
      },
      "execution_count": 231,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}