{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EFqPPMBkHBh"
   },
   "source": [
    "## Assignment 1\n",
    "### Author: Ilya Grigorev, DS-01\n",
    "\n",
    "In this assignment, I demonstrate a web retrieving pipeline that is capable of scraping data from a specific website, clean it, and store in the remote database. Additionally, a visualization frontend with customized features is developed to present captured information in a user-friendly format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfwPR1zZkHBi"
   },
   "source": [
    "## Install necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kzb-2PQukHBj",
    "outputId": "8e39a1db-85a8-446c-b739-4a6d746f6c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.12.2)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n",
      "Collecting pymongo\n",
      "  Downloading pymongo-4.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
      "Downloading pymongo-4.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymongo\n",
      "Successfully installed pymongo-4.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbt1kVrMkHBj"
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "QH0mM7kVkHBk"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, urlparse\n",
    "from urllib.error import HTTPError, URLError\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Tuple, List, TypedDict, Optional, Any, NamedTuple\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for scraping and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJ_YUnX6kHBk"
   },
   "outputs": [],
   "source": [
    "TARGET_URL = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n",
    "BASE_URL = '://'.join(urlparse(TARGET_URL)[:2])\n",
    "\n",
    "\n",
    "def extract_html(url: str) -> Any:\n",
    "    \"\"\"\n",
    "    Retrieves html content of a webpage.\n",
    "\n",
    "    Arguments:\n",
    "        url (str): url of a webpage.\n",
    "    \n",
    "    Returns:\n",
    "        html content of a webpage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        print(e.__str__())\n",
    "    except URLError:\n",
    "        print(\"The server could not be found\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def exclude_refs(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a string from excessive sources links, e.g. \"New York [1]\" -> \"New York\".\n",
    "\n",
    "    Arguments:\n",
    "        name (str): string.\n",
    "\n",
    "    Returns:\n",
    "        formatted string.\n",
    "    \"\"\"\n",
    "    pos = name.find('[')\n",
    "    return name[:pos] if pos > 0 else name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting main page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PM5PWkdwkHBl"
   },
   "outputs": [],
   "source": [
    "if (html := extract_html(TARGET_URL)) is not None:\n",
    "    main_page = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating table with target information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-AD1d0-kHBl"
   },
   "outputs": [],
   "source": [
    "highest_grossing_films = main_page.find('table', {'class': 'wikitable plainrowheaders sticky-header col4right col5center col6center'})\n",
    "assert highest_grossing_films is not None   # actually present\n",
    "assert len(highest_grossing_films.find_all('tr')) != 0  # has rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIHUv457kHBl"
   },
   "outputs": [],
   "source": [
    "class FilmRecord(TypedDict):\n",
    "    \"\"\"\n",
    "    Typed dictionary for representing a film.\n",
    "\n",
    "    Attributes:\n",
    "        title (str): title of a film.\n",
    "        release_year (int or None): release year of a film.\n",
    "        director (str or None): director/-s of a film, each separated by semicolon.\n",
    "        box_office (float or None): box office revenue of a film.\n",
    "        country (str or None): country/-es of origin of a film, each separated by semicolon.\n",
    "    \"\"\"\n",
    "    title: str\n",
    "    release_year: Optional[int]\n",
    "    director: Optional[str]\n",
    "    box_office: Optional[float]\n",
    "    country: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main extracting and cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpXQ0YELkHBl"
   },
   "outputs": [],
   "source": [
    "def parse_revenue(revenue: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Splits the revenue string into currency, value, and order.\n",
    "    Transforms value and order into a single number.\n",
    "\n",
    "    Arguments:\n",
    "        revenue (str): revenue in the specific format \"{currency}{number in scientific notation} {order}, e.g. \"$1.08 million\"\n",
    "    \n",
    "    Returns:\n",
    "        converted revenue value, e.g. 1080000.0\n",
    "    \n",
    "    \"\"\"\n",
    "    revenue = exclude_refs(revenue) # clean the string\n",
    "    quantity, order = revenue.split()   # split by space\n",
    "    value = quantity[re.search(\"[\\d\\.]+\", quantity).start():]\n",
    "    # Convert the value from scientific notation to a usual decimal number\n",
    "    if (order == 'million'):\n",
    "        major = ''.join(value.split('.'))\n",
    "        digits_after_decimal = len(value.split('.')[1])\n",
    "        return float(major + '0' * (6-digits_after_decimal))\n",
    "    elif (order == 'billion'):\n",
    "        major = ''.join(value.split('.'))\n",
    "        digits_after_decimal = len(value.split('.')[1])\n",
    "        return float(major + '0' * (9-digits_after_decimal))\n",
    "    else:\n",
    "        print(f\"Unresolved order: {order}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_film_page(film_url: str) -> Optional[Tuple[str, float, str]]:\n",
    "    \"\"\"\n",
    "    Parses information from a film page. Specifically, finds information about directors, box office revenue, and countries of origin.\n",
    "\n",
    "    Arguments:\n",
    "        film_url (str): url of a film.\n",
    "    \n",
    "    Returns:\n",
    "        tuple of directors string, box revenue, and countries or None if the page was not retrieved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieving webpage\n",
    "    if (html := extract_html(film_url)) is None:\n",
    "      return None\n",
    "    film_information = BeautifulSoup(html, 'html.parser').find('table', {'class': 'infobox vevent'})\n",
    "\n",
    "    # Directors information retrieval\n",
    "    directors_row_element = film_information.find(lambda tag: re.compile('^\\s*[Dd]irected\\s+by').match(tag.text) is not None)\n",
    "    if directors_row_element is not None:\n",
    "        # Type 1 format\n",
    "        if directors_row_element.find('div', {'class': 'plainlist'}) is not None:\n",
    "            directors_list_element = directors_row_element.find_all('div', {'class': 'plainlist'})[-1]\n",
    "            # organize into a single string separated by ';'\n",
    "            directors = ';'.join(exclude_refs(director_element.text) for director_element in directors_list_element.find_all('li'))\n",
    "        else:\n",
    "            # Type 2 format\n",
    "            annotated_text = directors_row_element.find_all()[1].get_text(strip=True, separator='\\n')\n",
    "            directors_list = re.split(\"\\[[^\\]]*\\]|\\n\", annotated_text)  # split by [, ], or \\n\n",
    "            directors = ';'.join(directors_list)    # organize into a single string separated by ';'\n",
    "    else:\n",
    "        print(f'Film (url={film_url}): directors list is not found')\n",
    "        directors = None\n",
    "\n",
    "    # Box office revenue retrieval\n",
    "    box_office_revenue_row_element = film_information.find(lambda tag: re.compile('^\\s*[Bb]ox\\s+office').match(tag.text) is not None)\n",
    "    if box_office_revenue_row_element is not None:\n",
    "        if (box_office_revenue_element:=box_office_revenue_row_element.find('td', {'class': 'infobox-data'})) is not None:\n",
    "            box_office_revenue = parse_revenue(box_office_revenue_element.text.strip())     # parsing revenue string\n",
    "        else:\n",
    "            print(f'Film (url={film_url}): box revenue is not found')\n",
    "            box_office_revenue = None\n",
    "    else:\n",
    "        print(f'Film (url={film_url}): box revenue is not found')\n",
    "        box_office_revenue = None\n",
    "\n",
    "    # Countries information retrieval\n",
    "    countries_row_element = film_information.find(lambda tag: re.compile('^\\s*[Cc]ountry|[Cc]ountries').match(tag.text) is not None)\n",
    "    if countries_row_element is not None:\n",
    "        # Type 1 format\n",
    "        if countries_row_element.find('div', {'class': 'plainlist'}) is not None:\n",
    "            country_list_element = countries_row_element.find_all('div', {'class': 'plainlist'})[-1]\n",
    "            countries = ';'.join(exclude_refs(country_element.text) for country_element in country_list_element.find_all('li')) # organize into a single string separated by ';'\n",
    "        else:\n",
    "            # Type 2 format\n",
    "            annotated_text = countries_row_element.find_all()[1].get_text(strip=True, separator='\\n')\n",
    "            countries_list = re.split(\"\\[[^\\]]*\\]|\\n\", annotated_text)  # split by [, ], or \\n\n",
    "            countries = ';'.join(countries_list)    # organize into a single string separated by ';'\n",
    "    else:\n",
    "        print(f'Film (url={film_url}): countries list is not found')\n",
    "        countries = None\n",
    "\n",
    "    return (directors, box_office_revenue, countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrUdq27-kHBm"
   },
   "outputs": [],
   "source": [
    "films: List[FilmRecord] = []    # retrieved data list\n",
    "film_rows = highest_grossing_films.find_all('tr')[1:]   # rows of the table with films, except the header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4yz0j8skHBm",
    "outputId": "5c286d1c-05ba-4223-a91a-e453b724192c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: Started processing\n",
      "Row 0: parsed a film\n",
      "Row 1: Started processing\n",
      "Row 1: parsed a film\n",
      "Row 2: Started processing\n",
      "Row 2: parsed a film\n",
      "Row 3: Started processing\n",
      "Row 3: parsed a film\n",
      "Row 4: Started processing\n",
      "Row 4: parsed a film\n",
      "Row 5: Started processing\n",
      "Row 5: parsed a film\n",
      "Row 6: Started processing\n",
      "Row 6: parsed a film\n",
      "Row 7: Started processing\n",
      "Row 7: parsed a film\n",
      "Row 8: Started processing\n",
      "Row 8: parsed a film\n",
      "Row 9: Started processing\n",
      "Row 9: parsed a film\n",
      "Row 10: Started processing\n",
      "Row 10: parsed a film\n",
      "Row 11: Started processing\n",
      "Row 11: parsed a film\n",
      "Row 12: Started processing\n",
      "Row 12: parsed a film\n",
      "Row 13: Started processing\n",
      "Row 13: parsed a film\n",
      "Row 14: Started processing\n",
      "Row 14: parsed a film\n",
      "Row 15: Started processing\n",
      "Row 15: parsed a film\n",
      "Row 16: Started processing\n",
      "Row 16: parsed a film\n",
      "Row 17: Started processing\n",
      "Row 17: parsed a film\n",
      "Row 18: Started processing\n",
      "Row 18: parsed a film\n",
      "Row 19: Started processing\n",
      "Row 19: parsed a film\n",
      "Row 20: Started processing\n",
      "Row 20: parsed a film\n",
      "Row 21: Started processing\n",
      "Row 21: parsed a film\n",
      "Row 22: Started processing\n",
      "Row 22: parsed a film\n",
      "Row 23: Started processing\n",
      "Row 23: parsed a film\n",
      "Row 24: Started processing\n",
      "Row 24: parsed a film\n",
      "Row 25: Started processing\n",
      "Row 25: parsed a film\n",
      "Row 26: Started processing\n",
      "Row 26: parsed a film\n",
      "Row 27: Started processing\n",
      "Row 27: parsed a film\n",
      "Row 28: Started processing\n",
      "Row 28: parsed a film\n",
      "Row 29: Started processing\n",
      "Row 29: parsed a film\n",
      "Row 30: Started processing\n",
      "Row 30: parsed a film\n",
      "Row 31: Started processing\n",
      "Row 31: parsed a film\n",
      "Row 32: Started processing\n",
      "Row 32: parsed a film\n",
      "Row 33: Started processing\n",
      "Row 33: parsed a film\n",
      "Row 34: Started processing\n",
      "Row 34: parsed a film\n",
      "Row 35: Started processing\n",
      "Row 35: parsed a film\n",
      "Row 36: Started processing\n",
      "Row 36: parsed a film\n",
      "Row 37: Started processing\n",
      "Row 37: parsed a film\n",
      "Row 38: Started processing\n",
      "Row 38: parsed a film\n",
      "Row 39: Started processing\n",
      "Row 39: parsed a film\n",
      "Row 40: Started processing\n",
      "Row 40: parsed a film\n",
      "Row 41: Started processing\n",
      "Row 41: parsed a film\n",
      "Row 42: Started processing\n",
      "Row 42: parsed a film\n",
      "Row 43: Started processing\n",
      "Row 43: parsed a film\n",
      "Row 44: Started processing\n",
      "Row 44: parsed a film\n",
      "Row 45: Started processing\n",
      "Row 45: parsed a film\n",
      "Row 46: Started processing\n",
      "Row 46: parsed a film\n",
      "Row 47: Started processing\n",
      "Row 47: parsed a film\n",
      "Row 48: Started processing\n",
      "Row 48: parsed a film\n",
      "Row 49: Started processing\n",
      "Row 49: parsed a film\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(film_rows):\n",
    "    # print(f\"Row {i}: Started processing\")\n",
    "    elements = row.find_all(recursive=False)\n",
    "    assert len(elements) == 6\n",
    "\n",
    "    # Title collection\n",
    "    title_element = elements[2].find('a')\n",
    "    if title_element is None:\n",
    "        # print(f\"Row {i}: title element was not found, the row is excluded\")\n",
    "        continue\n",
    "    title_link = title_element.attrs['href']\n",
    "    title = title_element.text.strip()\n",
    "\n",
    "    # Year collection\n",
    "    year_element = elements[4]\n",
    "    if len(year_element.text.strip()) == 0:\n",
    "        print(f\"Row {i}: release year is missing\")\n",
    "    try:\n",
    "        release_year = int(year_element.text.strip())\n",
    "    except ValueError:\n",
    "        release_year = None\n",
    "        print(f\"Row {i}: invalid year format: {year_element.text.strip()}\")\n",
    "\n",
    "    # Moving to film page\n",
    "    film_url = BASE_URL + title_link\n",
    "    director, box_office, country = parse_film_page(film_url)\n",
    "    print(f\"Row {i}: parsed a film\")\n",
    "    films.append(FilmRecord(title=title, release_year=release_year, director=director, box_office=box_office, country=country))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGjIRzqL3LVC"
   },
   "source": [
    "## Database creation\n",
    "\n",
    "In this assignment, I decided to use the MongoDB as the database to store data. Firstly, the database server provides remote access to the database without the need to host a server myself. Secondly, the document database is the best choice, as converting from python dict / json is straightforward (no need to specify fixed-form schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "r2Ud2_h2q861"
   },
   "outputs": [],
   "source": [
    "# Replace the placeholders with your actual MongoDB Atlas credentials\n",
    "username = \"pyclient\"\n",
    "password = \"admin\"\n",
    "URL = \"@mycluster.hszuy.mongodb.net/?retryWrites=true&w=majority&appName=MyCluster\"\n",
    "\n",
    "# Construct the MongoDB URI with authentication details\n",
    "mongo_uri = f\"mongodb+srv://{username}:{password}{URL}\"\n",
    "\n",
    "# Create a MongoClient object with the URI\n",
    "client = MongoClient(mongo_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating database and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1OKMoFsq-Ux"
   },
   "outputs": [],
   "source": [
    "db = client[\"wikipedia\"]\n",
    "if \"highest_grossing\" not in db.list_collection_names():  # if the collection is not present\n",
    "  collection = db[\"highest_grossing\"]\n",
    "  for film in films:\n",
    "      collection.insert_one(film)\n",
    "else:\n",
    "  collection = db[\"highest_grossing\"]\n",
    "  # collection.drop()   # uncomment to drop database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrDqiZNB8woL"
   },
   "source": [
    "## Exporting to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gc0h9Jcl6ADJ"
   },
   "outputs": [],
   "source": [
    "cursor = collection.find()\n",
    "\n",
    "# Converting cursor to the list of dictionaries\n",
    "list_cur = list(cursor)\n",
    "\n",
    "# Remove MongoDB id field\n",
    "for film in list_cur:\n",
    "  film.pop('_id')\n",
    "\n",
    "# Convert to json string\n",
    "json_data = json.dumps(list_cur, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save to a file\n",
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
